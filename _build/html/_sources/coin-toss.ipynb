{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06764a58",
   "metadata": {},
   "source": [
    "# Coin toss problem\n",
    "\n",
    "## What is the problem?\n",
    "\n",
    "Let's pick up a random coin (not necessarily a fair one with equal probability of head and tail). We did the coin toss experiment $n$ times and gathered the observed data $D$ as a set of outcomes (e.g. $\\{H, T, T, ...\\}$). Now, we are interested in predicting the probability of heads $p(H)=\\theta_{best}$ for our coin.\n",
    "\n",
    "## Applying Bayes rule\n",
    "\n",
    "In this problem, we will {ref}`model the distribution of parameters <parameters-framework>`. \n",
    "\n",
    "\\begin{equation}\n",
    "\\underbrace{p(\\theta|D)}_{\\text{Posterior}} = \\frac{\\overbrace{p(D|\\theta)}^{\\text{Likelihood}}}{\\underbrace{p(D)}_{\\text{Evidence}}}\\underbrace{p(\\theta)}_{\\text{Prior}}\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "p(D) = \\int_{\\theta}p(D|\\theta)p(\\theta)d\\theta\n",
    "\\end{equation}\n",
    "\n",
    "We are interested in $p(\\theta|D)$ and to derive that, we need prior, likelihood and evidence terms. Let us look at them one by one.\n",
    "\n",
    "### Prior\n",
    "\n",
    "What is our prior belief about the coin's probability of head $p($H$)$? Yes, that's exactly the question. A most simple way is to assume equal probability of heads and tails. However, we can represent our prior belief in terms of a distribution. Let's assume a beta distribution over the probability of heads $p(H) = \\theta$ (we will see in later sections why beta and not Gaussian or uniform or something else?). So, our prior distibution $p(\\theta)$ is:\n",
    "\n",
    "$$\n",
    "p(\\theta|\\alpha, \\beta) = \\frac{\\theta^{\\alpha-1}(1-\\theta)^{\\beta-1}}{B(\\alpha,\\beta)}, \\alpha,\\beta>0\\\\\n",
    "B(\\alpha, \\beta) = \\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha+\\beta)}\\\\\n",
    "\\Gamma(\\alpha) = (\\alpha-1)!\n",
    "$$\n",
    "\n",
    "Here, $\\alpha$ and $\\beta$ are the hyperparameters of the beta distrubution. $B$ is Beta function. You may play with [this interactive demo](https://huggingface.co/spaces/Zeel/Beta_distribution) to see how pdf changes with $\\alpha$ and $\\beta$. In our modeling, we can assume that $\\alpha$ and $\\beta$ are already known. There are methods of assuming distributions over the $\\alpha$ and $\\beta$ as well but that's out of the scope for now.\n",
    "\n",
    "### Likelihood\n",
    "\n",
    "Likelihood is probability of observing the data $D$ given $\\theta$. From, $n$ number of experiments, if we received heads $h$ times, then $p(D|\\theta)$ follows a Bernoulli distribution. We can also arrive at this formula by following the basic probability rules for independent events:\n",
    "\n",
    "$$\n",
    "p(D|\\theta) = \\theta^h(1-\\theta)^{n-h}\n",
    "$$\n",
    "\n",
    "### Maximum likelihood estimation (MLE)\n",
    "\n",
    "In cases, where prior is not available, we can use likelihood to get the best estimate of $\\theta$. Let us find the optimal theta by differentiating likelihood $p(D|\\theta)$ w.r.t $\\theta$.\n",
    "\n",
    "\\begin{align}\n",
    "p(D|\\theta) &= (\\theta)^h(1-\\theta)^{n-h}\\\\\n",
    "\\text{taking log both sides to simplify things,}\\\\\n",
    "\\log p(D|\\theta) &= h\\log(\\theta)+(n-h)\\log(1-\\theta)\\\\\n",
    "\\frac{d}{d\\theta}\\log p(D|\\theta) &= \\frac{h}{\\theta} - \\frac{n-h}{1-\\theta} = 0\\\\\n",
    "&= h(1-\\theta)-(n-h)\\theta = 0\\\\\n",
    "&= h - h\\theta - n\\theta + h\\theta = 0\\\\\n",
    "\\therefore \\theta_{MLE} = \\frac{h}{n}\n",
    "\\end{align}\n",
    "\n",
    "How can we know if optima at $\\theta_{MLE}$ is a maxima? well, it is a maxima if $\\frac{d^2}{d\\theta^2}\\log p(D|\\theta)$ is negative [(check here if not convinced)](https://www.khanacademy.org/math/multivariable-calculus/applications-of-multivariable-derivatives/optimizing-multivariable-functions/a/second-partial-derivative-test):\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{d}{d\\theta}\\log p(D|\\theta) &= \\frac{h}{\\theta} - \\frac{n-h}{1-\\theta}\\\\\n",
    "\\frac{d^2}{d\\theta^2}\\log p(D|\\theta) &= -\\frac{h}{\\theta^2}-\\frac{n-h}{(1-\\theta)^2}\n",
    "\\end{align}\n",
    "\n",
    "After a bit of thinking, one can see that above value is always negative and thus our optima is a maxima.\n",
    "\n",
    "### Maximum a posteriori estimation (MAP)\n",
    "\n",
    "We know that posterior is given by the following formula:\n",
    "\n",
    "\\begin{equation}\n",
    "\\underbrace{p(\\theta|D)}_{\\text{Posterior}} = \\frac{\\overbrace{p(D|\\theta)}^{\\text{Likelihood}}}{\\underbrace{p(D)}_{\\text{Evidence}}}\\underbrace{p(\\theta)}_{\\text{Prior}}\n",
    "\\end{equation}\n",
    "\n",
    "If we are only interested in maximum probable value of $\\theta$ in the posterior (point estimate in other words), we can differentiate the posterior w.r.t. $\\theta$. However, we have not yet derived the evidence but it does not depend on $\\theta$. So, we can claim that the following is true:\n",
    "\n",
    "$$\n",
    "\\arg \\max_{\\theta} p(\\theta|D) = \\arg \\max_{\\theta} p(D|\\theta)p(\\theta)\n",
    "$$\n",
    "\n",
    "Now, differentiating $p(D|\\theta)p(\\theta)$ w.r.t $\\theta$:\n",
    "\n",
    "\\begin{align}\n",
    "p(D|\\theta)p(\\theta) &= \\theta^h(1-\\theta)^{N-h}\\cdot\\frac{\\theta^{\\alpha-1}(1-\\theta)^{\\beta-1}}{B(\\alpha, \\beta)}\\\\\n",
    "                     &= \\frac{\\theta^{h+\\alpha-1}(1-\\theta)^{N-h+\\beta-1}}{B(\\alpha, \\beta)}\\\\\n",
    "\\text{Taking log for simplification}\\\\\n",
    "\\log p(\\theta|D)p(\\theta) &= (h+\\alpha-1)\\log(\\theta) + (N-h+\\beta-1)\\log(1-\\theta) - \\log(B(\\alpha, \\beta))\\\\\n",
    "\\\\\n",
    "\\frac{d}{d\\theta} \\log p(\\theta|D)p(\\theta) &= \\frac{h+\\alpha-1}{\\theta} - \\frac{N-h+\\beta-1}{1-\\theta} = 0\\\\\n",
    "\\\\\n",
    "\\therefore \\theta_{MAP} = \\frac{h+(\\alpha-1)}{N+(\\alpha-1)+(\\beta-1)}\n",
    "\\end{align}\n",
    "\n",
    "Now, we have the maximum probable value of $\\theta$ from the posterior but if we are interested in the posterior distribution, we must get the evidence!\n",
    "\n",
    "### Evidence\n",
    "\n",
    "The formula for computing the evidence is the following:\n",
    "\n",
    "$$\n",
    "p(D) = \\int\\limits_{\\theta}p(D|\\theta)p(\\theta)d\\theta\n",
    "$$\n",
    "\n",
    "Substituting the values and deriving the formula:\n",
    "\n",
    "\\begin{align}\n",
    "p(D) &= \\int\\limits_{0}^{1}p(D|\\theta)p(\\theta)d\\theta\\\\\n",
    "     &= \\int\\limits_{0}^{1}(\\theta)^h(1-\\theta)^{N-h}\\frac{\\theta^{\\alpha-1}(1-\\theta)^{\\beta-1}}{B(\\alpha,\\beta)}d\\theta\\\\\n",
    "     &= \\frac{1}{B(\\alpha,\\beta)}\\int\\limits_{0}^{1}(\\theta)^{h+\\alpha-1}(1-\\theta)^{N-h+\\beta-1}d\\theta\\\\\n",
    "     &= \\frac{1}{B(\\alpha,\\beta)}B(h+\\alpha, N-h+\\beta)\\\\\n",
    "     \\therefore p(D) = \\frac{B(h+\\alpha, N-h+\\beta)}{B(\\alpha,\\beta)}\n",
    "\\end{align}\n",
    "\n",
    "The last step follows from definition of [the Beta function](https://en.wikipedia.org/wiki/Beta_function).\n",
    "\n",
    "### Posterior\n",
    "\n",
    "Now, we have all the required terms to compute the posterior $p(\\theta|D)$.\n",
    "\n",
    "\\begin{align}\n",
    "p(\\theta|D) &= \\frac{p(D|\\theta)}{p(D)}p(\\theta)\\\\\n",
    "&= \\theta^h(1-\\theta)^{n-h} \\cdot \\frac{B(\\alpha,\\beta)}{B(h+\\alpha, N-h+\\beta)} \\cdot \\frac{\\theta^{\\alpha-1}(1-\\theta)^{\\beta-1}}{B(\\alpha,\\beta)}\\\\\n",
    "&= \\frac{\\theta^{h+\\alpha-1}(1-\\theta)^{N-h+\\beta-1}}{B(h+\\alpha, N-h+\\beta)}\n",
    "\\\\\n",
    "\\therefore p(\\theta|D) = Beta(h+\\alpha, N-h+\\beta)\n",
    "\\end{align}\n",
    "\n",
    "We have successfully derived the posterior and it follows a Beta distribution.\n",
    "\n",
    "## MAP is not the expected value of the posterior\n",
    "\n",
    "From [Wikipedia](https://en.wikipedia.org/wiki/Beta_distribution), expected value of our posterior is:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{\\theta}(p(\\theta|D)) = \\frac{h+\\alpha}{N + \\alpha + \\beta}\n",
    "$$\n",
    "\n",
    "We derived the MAP as:\n",
    "\n",
    "$$\n",
    "\\theta_{MAP} = \\frac{h+(\\alpha-1)}{N+(\\alpha-1)+(\\beta-1)}\n",
    "$$\n",
    "\n",
    "We can see that both values are clearly different. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
