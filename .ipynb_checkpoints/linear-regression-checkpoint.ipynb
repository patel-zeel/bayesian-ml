{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2bb725b6",
   "metadata": {},
   "source": [
    "# Bayesian Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d78240",
   "metadata": {},
   "source": [
    "## What is the problem?\n",
    "\n",
    "Given inputs $X$ and outputs $\\mathbf{y}$, we want to find the best parameters $\\boldsymbol{\\theta}$, such that predictions $\\hat{\\mathbf{y}} = X\\boldsymbol{\\theta}$ can estimate $\\mathbf{y}$ very well. In other words, we want L2 norm of errors $||\\hat{\\mathbf{y}} - \\mathbf{y}||_2$, as low as possible. \n",
    "\n",
    "## Applying Bayes Rule\n",
    "\n",
    "In this problem, we will {ref}`model the distribution of parameters <parameters-framework>`. \n",
    "\n",
    "\\begin{equation}\n",
    "\\underbrace{p(\\boldsymbol{\\theta}|X, \\mathbf{y})}_{\\text{Posterior}} = \\frac{\\overbrace{p(\\mathbf{y}|X, \\boldsymbol{\\theta})}^{\\text{Likelihood}}}{\\underbrace{p(\\mathbf{y}|X)}_{\\text{Evidence}}}\\underbrace{p(\\boldsymbol{\\theta})}_{\\text{Prior}}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "p(\\mathbf{y}|X) = \\int_{\\boldsymbol{\\theta}}p(\\mathbf{y}|X, \\boldsymbol{\\theta})p(\\boldsymbol{\\theta})d\\boldsymbol{\\theta}\n",
    "\\end{equation}\n",
    "\n",
    "We are interested in posterior $p(\\boldsymbol{\\theta}|X, \\mathbf{y})$ and to derive that, we need prior, likelihood and evidence terms. Let us look at them one by one.\n",
    "\n",
    "### Prior\n",
    "\n",
    "Let's assume a multivariate Gaussian prior over the $\\boldsymbol{\\theta}$ vector.\n",
    "\n",
    "$$\n",
    "p(\\theta) \\sim \\mathcal{N}(\\boldsymbol{\\mu}_0, \\Sigma_0)\n",
    "$$\n",
    "\n",
    "### Likelihood\n",
    "\n",
    "Given a $\\boldsymbol{\\theta}$, our prediction is $X\\boldsymbol{\\theta}$. Our data $\\mathbf{y}|X$ will have some irreducible noise which needs to be incorporated in the likelihood. Thus, we can assume the likelihood distribution over $\\mathbf{y}$ to be centered at $X\\boldsymbol{\\theta}$ with random i.i.d. homoskedastic noise with variance $\\sigma^2$:\n",
    "\n",
    "$$\n",
    "p(\\mathbf{y}|X, \\theta) \\sim \\mathcal{N}(X\\boldsymbol{\\theta}, \\sigma^2I)\n",
    "$$\n",
    "\n",
    "### Maximum Likelihood Estimation (MLE)\n",
    "\n",
    "Let us find the optimal parameters by differentiating likelihood $p(\\mathbf{y}|X, \\boldsymbol{\\theta})$ w.r.t $\\boldsymbol{\\theta}$.\n",
    "\n",
    "\\begin{equation}\n",
    "p(\\mathbf{y}|X, \\boldsymbol{\\theta}) = \\frac{1}{\\sqrt{(2\\pi)^n |\\sigma^2I|}}\\exp \\left( (\\mathbf{y} - X\\boldsymbol{\\theta})^T(\\sigma^2I)^{-1}(\\mathbf{y} - X\\boldsymbol{\\theta}) \\right)\n",
    "\\end{equation}\n",
    "\n",
    "Simplifying the above equation:\n",
    "\n",
    "\\begin{equation}\n",
    "p(\\mathbf{y}|X, \\boldsymbol{\\theta}) = \\frac{1}{(2\\pi\\sigma^2)^{\\frac{n}{2}}}\\exp \\left( \\sigma^{-2}(\\mathbf{y} - X\\boldsymbol{\\theta})^T(\\mathbf{y} - X\\boldsymbol{\\theta}) \\right)\n",
    "\\end{equation}\n",
    "\n",
    "Taking log to simplify further:\n",
    "\n",
    "\\begin{align}\n",
    "\\log p(\\mathbf{y}|X, \\boldsymbol{\\theta}) &= (\\mathbf{y} - X\\boldsymbol{\\theta})^T(\\mathbf{y} - X\\boldsymbol{\\theta}) + \\log \\sigma^{-2} + \\log \\frac{1}{(2\\pi\\sigma^2)^{\\frac{n}{2}}}\\\\\n",
    "\\frac{d}{d\\boldsymbol{\\theta}} \\log p(\\mathbf{y}|X, \\boldsymbol{\\theta}) &= \\frac{d}{d\\boldsymbol{\\theta}}(\\mathbf{y} - X\\boldsymbol{\\theta})^T(\\mathbf{y} - X\\boldsymbol{\\theta})\\\\\n",
    "&= \\frac{d}{d\\boldsymbol{\\theta}}(\\mathbf{y}^T - \\boldsymbol{\\theta}^TX^T)(\\mathbf{y} - X\\boldsymbol{\\theta})\\\\\n",
    "&= \\frac{d}{d\\boldsymbol{\\theta}} \\left[ \\mathbf{y}^T\\mathbf{y} - \\mathbf{y}^TX\\boldsymbol{\\theta} - \\boldsymbol{\\theta}^TX^T\\mathbf{y} + \\boldsymbol{\\theta}^TX^TX\\boldsymbol{\\theta}\\right]\\\\\n",
    "&= -(\\mathbf{y}^TX)^T - X^T\\mathbf{y} + 2X^TX\\boldsymbol{\\theta} = 0\\\\\n",
    "\\therefore X^TX\\boldsymbol{\\theta} &= X^T\\mathbf{y}\\\\\n",
    "\\therefore  \\boldsymbol{\\theta}_{MLE} &= (X^TX)^{-1}X^T\\mathbf{y}\n",
    "\\end{align}\n",
    "\n",
    "We used some of the formulas from [this cheatsheet](http://www.gatsby.ucl.ac.uk/teaching/courses/sntn/sntn-2017/resources/Matrix_derivatives_cribsheet.pdf) but they can also be derived from scratch.\n",
    "\n",
    "### Maximum a posteriori estimation (MAP)\n",
    "\n",
    "We know from {ref}`the previous discussion <MAP-1>` that:\n",
    "\n",
    "$$\n",
    "\\arg \\max_{\\boldsymbol{\\theta}} p(\\boldsymbol{\\theta}|X, \\mathbf{y}) = \\arg \\max_{\\boldsymbol{\\theta}} p(\\mathbf{y}|X, \\boldsymbol{\\theta})p(\\boldsymbol{\\theta})\n",
    "$$\n",
    "\n",
    "Now, differentiating $p(\\mathbf{y}|X, \\boldsymbol{\\theta})p(\\boldsymbol{\\theta})$ w.r.t $\\theta$ by reusing some of the steps from MLE:\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
